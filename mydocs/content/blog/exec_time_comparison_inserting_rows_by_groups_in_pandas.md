+++
title = "ðŸ‰ Casual execution time comparison: updating rows per group in Pandas"
description = ""
date = "2024-02-09T03:28:57+09:00"
updated = "2024-02-09T03:28:57+09:00"
draft = true
template = "blog/page.html"

[taxonomies]
authors = ["snrsw"]

[extra]
lead = "Lightweight comparison of the execution time of different methods for updating rows per group in Pandas."
+++

# TL;DR

In this post, we consider the efficient methods of updating rows per group in Pandas. We compare the average execution time of different methods for different sizes of data to find the most efficient method.
The result shows that the map method is the most efficient.

# Problem and Goals

The problem of updating rows per group is described as follows:

Input:

|id|value|
|---|---|
|A|1|
|A|2|
|B|3|
|B|4|
|C|5|

Task:
- Update the value per group
  - Let us update the value of id A to 10, id B to 20, and id C to 30.

Output:

|id|value|
|---|---|
|A|10|
|A|10|
|B|20|
|B|20|
|C|30|


The goal of this post is to compare the efficiency of different methods for updating rows per group in Pandas.

# Experiment

## Methods of updating rows per group

We consider the following methods for updating rows per group in Pandas:

1. Using `pd.DataFrame.loc`
2. Using `pd.DataFrame.groupby`
3. Using `pd.DataFrame.apply`
4. Using `pd.DataFrame.mask`
5. Using `np.where`
6. Using `map`

(I think these are common methods for updating rows per group in Pandas. If you know other efficient methods, please let me know.)

In detail, the following functions are used for each method: Note that `id2newvalue` is a dictionary that maps the id to the new value.

### Using `pd.DataFrame.loc`

```python
def use_loc(df: pd.DataFrame, id2newvalue: dict[int, int]) -> None:
    for id in df["id"].unique():
        df.loc[df["id"] == id, "value"] = id2newvalue[id]
    return df
```

### Using `pd.DataFrame.groupby`

```python
def use_groupby(df: pd.DataFrame, id2newvalue: dict[int, int]) -> None:
    return pd.concat(
        [
            gdf.assign(
                value=id2newvalue[g]
            )
            for g, gdf in df.groupby("id")
        ]
    )
```

### Using `pd.DataFrame.apply`

```python
def use_apply(df: pd.DataFrame, id2newvalue: dict[int, int]):
    df["value"] = df["id"].apply(lambda x: id2newvalue[x])
    return df
```

### Using `pd.DataFrame.mask`

```python
def use_pd_mask(df: pd.DataFrame, id2newvalue: dict[int, int]):
    for id in df["id"].unique():
        df["value"].mask(df["id"] == id, id2newvalue[id], inplace=True)
    return df
```

### Using `np.where`

```python
def use_np_where(df: pd.DataFrame, id2newvalue: dict[int, int]):
    for id in df["id"].unique():
        df["value"] = np.where(df["id"] == id, id2newvalue[id], df["value"])
    return df
```

### Using `map`

```python
def use_map(df: pd.DataFrame, id2newvalue: dict[int, int]):
    df["value"] = df["id"].map(id2newvalue)
    return df
```

### Comparison Methods

We compare the average execution time of the above methods for different sizes of data.

The average execution time is calculated by the following functions:

```python
# use as a decorator to measure the execution time of a function
def log_exec_time(f: Callable) -> Callable:
    def _wrapper(*args: Any, **kwargs: Any) -> Any:
        start = datetime.datetime.now()

        v = f(*args, **kwargs)

        end = datetime.datetime.now()
        exec_time = end - start
        print(f"{f.__name__} execution time: {exec_time}")
        return v, exec_time
    return _wrapper

# calculate the average execution time of a function decorated by `log_exec_time`
def average_exec_time(
    df: pd.DataFrame,
    id2newvalue: dict[int, int],
    f: Callable,
    n_experiments: int = 10
) -> datetime.timedelta:
    exec_times = []
    for _ in range(n_experiments):
        result, exec_time = f(df=df.copy(), id2newvalue=id2newvalue)
        exec_times.append(exec_time)

    exec_time_mean = sum(exec_times, datetime.timedelta(0)) / len(exec_times)
    return exec_time_mean

```

The size of the data is defined as a pair of the number of unique ids and the number of rows per id. The data is generated by the following functions:

```python
def generate_dataframe(n_ids: int, n_rows_per_id: int) -> pd.DataFrame:
    return pd.concat([
        pd.DataFrame(
            {
                "id": [id] * n_rows_per_id,
                "value": [id] * n_rows_per_id
            }
        )
        for id in range(n_ids)
    ])
```

```python
def generate_id2newvalue(dataframe: pd.DataFrame) -> dict[int, int]:
    return {
        _: _ * 2
        for _ in dataframe["id"].unique()
    }
```

## Results

|   n_ids |   n_rows_per_id |   n_experiments |   loc_exec_time |   groupby_exec_time |   apply_exec_time |   pd_mask_exec_time |   np_where_exec_time |   map_exec_time |
|--------:|----------------:|---------------:|----------------:|--------------------:|------------------:|--------------------:|---------------------:|----------------:|
|      10 |              10 |             10 |           3.812 |               3.743 |             1.484 |               7.325 |                3.185 |           0.936 |
|      10 |             100 |             10 |           3.732 |               3.8   |             3.968 |               8.65  |                4.157 |           1.705 |
|      10 |            1000 |             10 |           4.633 |               4.506 |            30.338 |               6.556 |                2.888 |           1.148 |
|     100 |              10 |             10 |          33.925 |              23.513 |             3.502 |              74.271 |               43.083 |           1.748 |
|     100 |             100 |             10 |          35.516 |              24.792 |            30.606 |              67.025 |               25.866 |           1.969 |
|     100 |            1000 |             10 |          76.029 |              31.685 |           416.172 |              88.715 |               52.532 |           4.256 |
|    1000 |              10 |             10 |         350.798 |             275.204 |            31.261 |             718.937 |              241.797 |           6.976 |
|    1000 |             100 |             10 |         666.854 |             331.496 |           299.176 |             960.2   |              524.357 |           8.438 |
|    1000 |            1000 |             10 |        3913.51  |             385.758 |          3289.46  |            3783.44  |             4967.53  |          30.878 |

exec time unit: ms


- The map method is the most efficient method,
- The groupby method is relatively efficient, especially the case where n_ids=1000 and n_rows_per_id=1000 is not bad,
- The other methods and are less efficient than the map and groupby methods,
- The number of unique ids has a strong effect on the efficiency of the methods.

# References

For more details, please see the following notebook:
[Google Colabratory](https://colab.research.google.com/drive/1j5BHKRA1VfLR00HPgoEFy_Sfz6UwWtX0?usp=sharing).
